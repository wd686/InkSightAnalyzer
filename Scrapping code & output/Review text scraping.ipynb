{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab8acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977c139",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367beb0-92f5-4fc1-a0f8-7c868f6d225e",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Need to download docker for scraping, please follow this video: https://www.youtube.com/watch?v=8q2K41QC2nQ&t=269s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d40534a-9e81-484d-b7b1-0e5072e9d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_source_site(title):\n",
    "    countries = {\n",
    "        \"com\": \"US\",\n",
    "        \"es\": \"Spain\",\n",
    "        \"it\": \"Italy\",\n",
    "        \"co.uk\": \"UK\",\n",
    "        \"de\": \"Germany\",\n",
    "        \"fr\": \"France\"\n",
    "    }\n",
    "    for code, country in countries.items():\n",
    "        if f\"Amazon.{code}\" in title:\n",
    "            return country\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a7e5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):   \n",
    "    r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})  \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')  \n",
    "    return soup  \n",
    "  \n",
    "def amazon_review(soup, url):    \n",
    "    title = soup.title.text\n",
    "    # print(title)\n",
    "    amazon_site = amazon_source_site(title)\n",
    "    # print(amazon_site)\n",
    "    review = {}\n",
    "    extracted_reviews = []   \n",
    "    try:\n",
    "        model = title.replace(\"Amazon.com: Customer reviews: \", \"\") \\\n",
    "                    .replace(\"Amazon.co.uk:Customer reviews: \" if amazon_site == 'UK' else \"\", \"\") \\\n",
    "                    .replace(\"Amazon.de:Customer Reviews: \" if amazon_site == 'Germany' else \"\", \"\") \\\n",
    "                    .replace(\"Amazon.it:Recensioni clienti: \" if amazon_site == 'Italy' else \"\", \"\") \\\n",
    "                    .replace(\"Amazon.es:Opiniones de clientes: \" if amazon_site == 'Spain' else \"\", \"\") \\\n",
    "                    .replace(\"Amazon.fr\\xa0:Commentaires en ligne: \" if amazon_site == 'France' else \"\", \"\").strip()\n",
    "    except AttributeError: \n",
    "        try:\n",
    "            model = soup.find(\"a\", attrs={\"data-hook\": \"product-link\"})\n",
    "        except AttributeError: \n",
    "            model = soup.find(\"h1\", attrs={\"class\": \"a-size-large a-text-ellipsis\"}).string.strip()  \n",
    "    # print('title:', model)\n",
    "    try:\n",
    "        Review_average = soup.find(\"i\", attrs={'class':'a-icon a-icon-star-medium a-star-medium-4-5 averageStarRating'}).string.strip().replace(' out of 5 stars', '') \\\n",
    "                                .replace(' su 5 stelle' if amazon_site == 'Italy' else \"\", \"\") \\\n",
    "                                .replace(' de 5 estrellas' if amazon_site == 'Spain' else \"\", \"\") \\\n",
    "                                .replace(' sur 5\\xa0étoiles' if amazon_site == 'France' else \"\", \"\").replace(',', '.')\n",
    "    except AttributeError:\n",
    "        Review_average = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.replace(' out of 5 stars', '') \\\n",
    "                                .replace(' su 5 stelle' if amazon_site == 'Italy' else \"\", \"\") \\\n",
    "                                .replace(' de 5 estrellas' if amazon_site == 'Spain' else \"\", \"\") \\\n",
    "                                .replace(' sur 5\\xa0étoiles' if amazon_site == 'France' else \"\", \"\").replace(',', '.')\n",
    "    # print(Review_average)\n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "    # Dictionary to map the site to the prefix\n",
    "    review_prefixes_and_delimiters = {\n",
    "        'Spain': ('Revisado en', ' el '),\n",
    "        'Italy': ('Recensito in', ' il '),\n",
    "        'France': ('Commenté en', ' le '),\n",
    "        'UK': ('Reviewed in', ' on '),\n",
    "        'US': ('Reviewed in', ' on '),\n",
    "        'Germany': ('Reviewed in', ' on ')\n",
    "    }\n",
    "    if reviews:\n",
    "        for item in reviews:    \n",
    "            review_details = item.find('span', {'data-hook': 'review-date'}).text\n",
    "            prefix, delimiter = review_prefixes_and_delimiters.get(amazon_site, ('Reviewed in', ' on '))\n",
    "            review_details = review_details.replace(prefix, '').strip()\n",
    "            \n",
    "            if delimiter in review_details:\n",
    "                date_country = review_details.split(delimiter)\n",
    "                review_country = date_country[0].strip()\n",
    "                review_date = date_country[1].strip()\n",
    "            else:\n",
    "                review_country = amazon_site  # Fallback to amazon_site\n",
    "                review_date = review_details.strip()\n",
    "            \n",
    "            review = {\n",
    "                'Amazon site': amazon_site,\n",
    "                'Model': model,    \n",
    "                'Review average' : Review_average,\n",
    "                'Review date': review_date,    \n",
    "                \"Review content\": item.find(\"span\", {'data-hook': \"review-body\"}).text.strip(),  \n",
    "                \"Review country\": review_country,\n",
    "                \"URL\": url\n",
    "            }\n",
    "            \n",
    "            try:    \n",
    "                review[\"Review rating\"] = float(item.find(\"i\", {\"data-hook\": \"review-star-rating\"}).text.replace(\"out of 5 stars\", \"\") \\\n",
    "                                                .replace(' su 5 stelle' if amazon_site == 'Italy' else \"\", \"\") \\\n",
    "                                                .replace(' de 5 estrellas' if amazon_site == 'Spain' else \"\", \"\") \\\n",
    "                                                .replace(' sur 5\\xa0étoiles' if amazon_site == 'France' else \"\", \"\").replace(',', '.').strip())    \n",
    "            except AttributeError:    \n",
    "                review[\"Review rating\"] = float(item.find(\"span\", {\"class\": \"a-icon-alt\"}).text.replace(\"out of 5 stars\", \"\") \\\n",
    "                                                .replace(' su 5 stelle' if amazon_site == 'Italy' else \"\", \"\") \\\n",
    "                                                .replace(' de 5 estrellas' if amazon_site == 'Spain' else \"\", \"\") \\\n",
    "                                                .replace(' sur 5\\xa0étoiles' if amazon_site == 'France' else \"\", \"\").replace(',', '.').strip())    \n",
    "            # print(review[\"Review rating\"])\n",
    "            try:    \n",
    "                review['Review title']  = item.find(\"a\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "            except AttributeError:    \n",
    "                review['Review title']  = item.find(\"span\", {'data-hook': \"review-title\"}).text.strip()    \n",
    "            # print(review['Review title'])\n",
    "      \n",
    "            # try:    \n",
    "            #     review[\"Verified Purchase or not\"] = item.find(\"span\", {'data-hook': \"avp-badge\"}).text.strip()    \n",
    "            # except AttributeError:    \n",
    "            #     review[\"Verified Purchase or not\"] = None    \n",
    "            # print(review[\"Verified Purchase or not\"])\n",
    "      \n",
    "            # try:      \n",
    "            #     review[\"Review name\"] = item.find(\"span\", {'class': \"a-profile-name\"}).string.strip()  \n",
    "            # except AttributeError:        \n",
    "            #     review[\"Review name\"] = None  \n",
    "      \n",
    "            # try:    \n",
    "            #     review[\"People find helpful\"] = item.find(\"span\", {'data-hook': \"helpful-vote-statement\"}).text.replace(\"people found this helpful\", \"\").strip()    \n",
    "            # except AttributeError:    \n",
    "            #     review[\"People find helpful\"] = None  \n",
    "                \n",
    "            # try:\n",
    "            #     review['Seeding or not'] = item.find(\"span\", {'class': \"a-size-mini a-color-link c7yBadgeAUI c7yTopDownDashedStrike c7y-badge-text a-text-normal c7y-badge-link c7y-badge-vine-voice a-text-bold\"}).text.strip() \n",
    "            # except AttributeError:   \n",
    "            #     review['Seeding or not'] = None      \n",
    "            extracted_reviews.append(review)    \n",
    "        # print(review)\n",
    "    else:\n",
    "        review = {    \n",
    "                'Amazon site': amazon_site,\n",
    "                'Model': model,    \n",
    "                'Review average' : Review_average,\n",
    "                \"URL\" : url  \n",
    "            }\n",
    "        extracted_reviews.append(review)  \n",
    "  \n",
    "    return extracted_reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27318c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f67b2b-a51e-4dfb-8c04-7a81d1d9b548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.com/HP-936-Cyan-Original-Cartridge/product-reviews/B0CJCD2TWT',\n",
       " 'https://www.amazon.com/HP-936-Magenta-Original-Cartridge/product-reviews/B0CJCDSS9Q',\n",
       " 'https://www.amazon.com/HP-936-Yellow-Original-Cartridge/product-reviews/B0CJCDRYDD',\n",
       " 'https://www.amazon.com/HP-936-Black-Original-Cartridge/product-reviews/B0CJCDJ88M',\n",
       " 'https://www.amazon.com/product-reviews/B0CJCG8QKB/',\n",
       " 'https://www.amazon.com/HP-936e-Cyan-EvoMore-Cartridge/product-reviews/B0CRD36HPR',\n",
       " 'https://www.amazon.com/HP-936e-Magenta-EvoMore-Cartridge/product-reviews/B0CRD5DLVJ',\n",
       " 'https://www.amazon.com/HP-936e-Yellow-EvoMore-Cartridge/product-reviews/B0CRD9J24L',\n",
       " 'https://www.amazon.com/HP-936e-Black-EvoMore-Cartridge/product-reviews/B0CRCW5FGN',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CRCHD5D5',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CRCP133B',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CRC61YBM',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CRCCHBD3',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CLM1FS55',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ177L3',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ14V4R',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ2S8T1',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ2MRMP',\n",
       " 'https://www.amazon.de/product-reviews/B0CRCHD5D5',\n",
       " 'https://www.amazon.de/product-reviews/B0CRCP133B',\n",
       " 'https://www.amazon.de/product-reviews/B0CRC61YBM',\n",
       " 'https://www.amazon.de/product-reviews/B0CJ9PQZF2',\n",
       " 'https://www.amazon.de/product-reviews/B0CLM1FS55',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ177L3',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ14V4R',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ2S8T1',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ2MRMP',\n",
       " 'https://www.amazon.it/product-reviews/B0CRCHD5D5',\n",
       " 'https://www.amazon.it/product-reviews/B0CRCP133B',\n",
       " 'https://www.amazon.it/product-reviews/B0CRC61YBM',\n",
       " 'https://www.amazon.it/product-reviews/B0CJ9PQZF2',\n",
       " 'https://www.amazon.it/product-reviews/B0CLM1FS55',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ177L3',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ14V4R',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ2S8T1',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ2MRMP',\n",
       " 'https://www.amazon.es/product-reviews/B0CRCHD5D5',\n",
       " 'https://www.amazon.es/product-reviews/B0CRCP133B',\n",
       " 'https://www.amazon.es/product-reviews/B0CRC61YBM',\n",
       " 'https://www.amazon.es/product-reviews/B0CJ9PQZF2',\n",
       " 'https://www.amazon.es/product-reviews/B0CLM1FS55',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ177L3',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ14V4R',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ2S8T1',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ2MRMP',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRCHD5D5',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRCP133B',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRC61YBM',\n",
       " 'https://www.amazon.fr/product-reviews/B0CJ9PQZF2',\n",
       " 'https://www.amazon.fr/product-reviews/B0CLM1FS55',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ177L3',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ14V4R',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ2S8T1',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ2MRMP',\n",
       " 'https://www.amazon.com/923-Cyan-Original-Ink-Cartridge/product-reviews/B0CKLWWV4M',\n",
       " 'https://www.amazon.com/923-Magenta-Original-Ink-Cartridge/product-reviews/B0CKLXDPY4',\n",
       " 'https://www.amazon.com/923-Yellow-Original-Ink-Cartridge/product-reviews/B0CKLW6TJ4',\n",
       " 'https://www.amazon.com/923-Black-Original-Ink-Cartridge/product-reviews/B0CKLX7K1X',\n",
       " 'https://www.amazon.com/product-reviews/B0CY1CSFHK',\n",
       " 'https://www.amazon.com/HP-923-Original-Ink-Cartridge/product-reviews/B0CKLZ2336',\n",
       " 'https://www.amazon.com/product-reviews/B0CRVYKWWS',\n",
       " 'https://www.amazon.com/product-reviews/B0CRVW9B1N',\n",
       " 'https://www.amazon.com/product-reviews/B0CRVWFPDN',\n",
       " 'https://www.amazon.com/product-reviews/B0CRVWD1M9',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CR6PH6NJ',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CR6Q88Y7',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CR6RKKRK',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CR6RB1TK',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CV1V759V',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ13G1H',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ4TF8J',\n",
       " 'https://www.amazon.co.uk/product-reviews/B0CXQ2CT6H',\n",
       " 'https://www.amazon.de/product-reviews/B0CRC61147',\n",
       " 'https://www.amazon.de/product-reviews/B0CRCMTXZG',\n",
       " 'https://www.amazon.de/product-reviews/B0CRCF98NJ',\n",
       " 'https://www.amazon.de/product-reviews/B0CRCCHY6X',\n",
       " 'https://www.amazon.de/product-reviews/B0CV1V759V',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ13G1H',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ4TF8J',\n",
       " 'https://www.amazon.de/product-reviews/B0CXQ2CT6H',\n",
       " 'https://www.amazon.de/product-reviews/B0CG96WW92',\n",
       " 'https://www.amazon.it/product-reviews/B0CRC61147',\n",
       " 'https://www.amazon.it/product-reviews/B0CRCMTXZG',\n",
       " 'https://www.amazon.it/product-reviews/B0CRCF98NJ',\n",
       " 'https://www.amazon.it/product-reviews/B0CRCCHY6X',\n",
       " 'https://www.amazon.it/product-reviews/B0CV1V759V',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ13G1H',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ4TF8J',\n",
       " 'https://www.amazon.it/product-reviews/B0CXQ2CT6H',\n",
       " 'https://www.amazon.it/product-reviews/B0CG96WW92',\n",
       " 'https://www.amazon.es/product-reviews/B0CRC61147',\n",
       " 'https://www.amazon.es/product-reviews/B0CRCMTXZG',\n",
       " 'https://www.amazon.es/product-reviews/B0CRCF98NJ',\n",
       " 'https://www.amazon.es/product-reviews/B0CRCCHY6X',\n",
       " 'https://www.amazon.es/product-reviews/B0CV1V759V',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ13G1H',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ4TF8J',\n",
       " 'https://www.amazon.es/product-reviews/B0CXQ2CT6H',\n",
       " 'https://www.amazon.es/product-reviews/B0CG96WW92',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRC61147',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRCMTXZG',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRCF98NJ',\n",
       " 'https://www.amazon.fr/product-reviews/B0CRCCHY6X',\n",
       " 'https://www.amazon.fr/product-reviews/B0CV1V759V',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ13G1H',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ4TF8J',\n",
       " 'https://www.amazon.fr/product-reviews/B0CXQ2CT6H',\n",
       " 'https://www.amazon.fr/product-reviews/B0CG96WW92']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read url\n",
    "path =  r\"Review text scrape URL.xlsx\"\n",
    "sheets = 'Amazon'\n",
    "amazon_url = pd.read_excel(path, sheet_name = sheets)\n",
    "urls = amazon_url['URL'].to_list()\n",
    "url_list = []\n",
    "for value in urls:\n",
    "    if pd.notna(value):\n",
    "        url_list.append(value)\n",
    "url_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5985f-bb08-4dcf-8511-74285d38e78f",
   "metadata": {},
   "source": [
    "## Scrape Reviews\n",
    "- Only last 10 pages of most recent reviews are retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e57c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "  \n",
    "\n",
    "star = ['one','two','three','four','five'] \n",
    "all_reviews = []  \n",
    "\n",
    "for link in url_list:    \n",
    "    print(link)  \n",
    "    for y in star:  \n",
    "        should_continue = True  \n",
    "        for x in range(1, 11):  \n",
    "            if not should_continue:  \n",
    "                break   \n",
    "            while True:\n",
    "                # while True:\n",
    "                try:  \n",
    "                    # url = f'{link}/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber={x}&filterByStar={y}_star'    \n",
    "                    # url = f'{link}/ref=cm_cr_arp_d_viewopt_sr?filterByStar={y}_star&pageNumber={x}'\n",
    "                    # sort by Most recent reviews\n",
    "                    url = f'{link}/ref=cm_cr_arp_d_viewopt_srt?ie=UTF8&filterByStar={y}_star&pageNumber={x}&sortBy=recent'\n",
    "                    soup = get_soup(url)    \n",
    "    \n",
    "                    # print(f'Getting page: {x}')    \n",
    "                    extracted_reviews = amazon_review(soup, url)   \n",
    "                    print(f'Extracted reviews on page {x}: {len(extracted_reviews)},{y}star')\n",
    "                    \n",
    "                    all_reviews.extend(extracted_reviews)   \n",
    "\n",
    "                    next_page_link = soup.find('li', {'class': 'a-last'})\n",
    "    \n",
    "                    if next_page_link is None or soup.find('li', {'class': 'a-disabled a-last'}):\n",
    "                        should_continue = False\n",
    "                        # print('No more pages left')\n",
    "                        break \n",
    "                    else:   \n",
    "                        break\n",
    "\n",
    "                except Exception as e:  \n",
    "                    print(f\"Error encountered: {e}. Retrying in 3 seconds...\")  \n",
    "                    time.sleep(10)  \n",
    "\n",
    "end_time = time.time()  \n",
    "\n",
    "elapsed_time = end_time - start_time  \n",
    "  \n",
    "print(f'Elapsed time: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06565145-867c-4a7a-8f84-7fe975c7d28a",
   "metadata": {},
   "source": [
    "## Translating the date from Spanish / Italian / French to English \n",
    "- Allows us to convert to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d7dc86-f32c-4914-b0fd-c4ee57223c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date \n",
    "pd.set_option('display.max_columns', None)\n",
    "amazon2 = pd.DataFrame(all_reviews)\n",
    "\n",
    "import re\n",
    "\n",
    "month_translation = {\n",
    "    'Spain': {\n",
    "        'enero': 'January', 'febrero': 'February', 'marzo': 'March', 'abril': 'April', \n",
    "        'mayo': 'May', 'junio': 'June', 'julio': 'July', 'agosto': 'August', \n",
    "        'septiembre': 'September', 'octubre': 'October', 'noviembre': 'November', 'diciembre': 'December'\n",
    "    },\n",
    "    'France': {\n",
    "        'janvier': 'January', 'février': 'February', 'mars': 'March', 'avril': 'April', \n",
    "        'mai': 'May', 'juin': 'June', 'juillet': 'July', 'août': 'August', \n",
    "        'septembre': 'September', 'octobre': 'October', 'novembre': 'November', 'décembre': 'December'\n",
    "    },\n",
    "    'Italy': {\n",
    "        'gennaio': 'January', 'febbraio': 'February', 'marzo': 'March', 'aprile': 'April', \n",
    "        'maggio': 'May', 'giugno': 'June', 'luglio': 'July', 'agosto': 'August', \n",
    "        'settembre': 'September', 'ottobre': 'October', 'novembre': 'November', 'dicembre': 'December'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to translate dates\n",
    "def translate_date(date_str, language):\n",
    "    if pd.isna(date_str) or date_str.strip() == '':\n",
    "        return None\n",
    "    if language in month_translation:\n",
    "        pattern1 = r'(\\d{1,2}) de (\\w+) de (\\d{4})'\n",
    "        pattern2 = r'(\\d{1,2})\\s+(?:de\\s+)?(\\w+)\\s+(\\d{4})'\n",
    "        match1 = re.match(pattern1, date_str)\n",
    "        match2 = re.match(pattern2, date_str)\n",
    "        if match1:\n",
    "            day, month, year = match1.groups()\n",
    "            month = month_translation[language].get(month.lower(), month)\n",
    "            return f\"{day} {month.capitalize()} {year}\"\n",
    "        elif match2:\n",
    "            day, month, year = match2.groups()\n",
    "            month = month_translation[language].get(month.lower(), month)\n",
    "            return f\"{day} {month.capitalize()} {year}\"\n",
    "    return date_str\n",
    "def translate_dates(row):\n",
    "    country = row['Amazon site']\n",
    "    if country in month_translation:\n",
    "        row['Review date'] = translate_date(row['Review date'], country)\n",
    "    return row\n",
    "    \n",
    "amazon2['Review date'] = amazon2['Review date'].astype(str)\n",
    "amazon2 = amazon2.apply(translate_dates, axis=1)\n",
    "\n",
    "# Function to convert multiple date formats\n",
    "def convert_to_datetime(date_str):\n",
    "    formats = [\"%B %d, %Y\", \"%d %B %Y\"]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "amazon2['Review date'] = amazon2['Review date'].apply(lambda x: convert_to_datetime(x) if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63d33d-98e3-424f-87ee-f2b3f8fe8949",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66a5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon2['Retailer']=\"Amazon\"\n",
    "# amazon2['Scraping Date'] = pd.to_datetime(date.today())\n",
    "# amazon2['Review date'] = amazon2['Review date'].str.strip()\n",
    "# amazon2['Review date'] = pd.to_datetime(amazon2['Review date'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "def extract_title(title):\n",
    "    if isinstance(title, str):\n",
    "        match = re.match(r'.*?\\n(.*)', title)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return title\n",
    "amazon2['Review title'] = amazon2['Review title'].apply(extract_title)\n",
    "\n",
    "# amazon2['Review title'] = amazon2['Review title'].str.replace(r'^.*?\\n', '')\n",
    "# amazon2['Review title'] = amazon2['Review title'].str.strip()\n",
    "# amazon2['Review title'] = amazon2['Review title'].str.extract(r'out of 5 stars\\n(.*)')\n",
    "amazon2['HP Model Number'] = amazon2['Model'].str.extract(r'(\\d+e?)')\n",
    "# amazon2['People find helpful'] = amazon2['People find helpful'].str.extract(r'(\\d*) people found this helpful')\n",
    "  \n",
    "selected_columns = ['Retailer', 'Amazon site', 'HP Model Number', 'Model', 'Review average', 'Review date', 'Review title', 'Review rating', \n",
    "                    'Review content', 'Review country', 'URL']\n",
    "  \n",
    "amazon_review = amazon2[selected_columns]  \n",
    "\n",
    "amazon_review = amazon_review.drop_duplicates()\n",
    "\n",
    "# Remove rows with no reviews and review scores\n",
    "amazon_review['Review average'] = pd.to_numeric(amazon_review['Review average'], errors='coerce')\n",
    "amazon_review = amazon_review[amazon_review['Review average'] != 0.0]\n",
    "\n",
    "# Remove rows with repeated data\n",
    "def remove_repeated(data):\n",
    "    non_empty_rows = data.dropna(subset=['Review date', 'Review title', 'Review rating', 'Review content'], how='all')\n",
    "    if not non_empty_rows.empty:\n",
    "        # If there are non-empty rows, drop all rows with any empty field\n",
    "       return non_empty_rows\n",
    "    else:\n",
    "        # If all rows have empty fields, keep only the first row\n",
    "        return data.head(1)\n",
    "\n",
    "# Apply the function to each model\n",
    "amazon_review = amazon_review.groupby('Model').apply(remove_repeated).reset_index(drop=True)\n",
    "\n",
    "\n",
    "date = datetime.today().strftime('%Y%m%d')  \n",
    "amazon_review.to_excel(f'scraped_reviews/amazon_reviews_{date}_test1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66853c",
   "metadata": {},
   "source": [
    "# Best Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b9f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_bestbuy(url):\n",
    "    extracted_reviews = []\n",
    "    retry_count = 0\n",
    "    header = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.61',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Cookie': 'SID=5dd8d974-1010-4705-9db0-0091b9be90eb; bby_rdp=l; CTT=422cf77c62f741992b73b7eb194dd19d; intl_splash=false; intl_splash=false; vt=d36b7cc9-70f1-11ee-af65-0a4fc06e3e21; rxVisitor=169798943988975DRVD09AP9VHNKB488A7AMQ2ITCSNQ3; COM_TEST_FIX=2023-10-22T15%3A44%3A00.270Z; __gads=ID=6d604286666986e7:T=1697989449:RT=1697989449:S=ALNI_Mb_Z6tWUAT9d1smc0S2VYNtEXVnJQ; __gpi=UID=00000c6de2768122:T=1697989449:RT=1697989449:S=ALNI_MY8b96wWX_3ahxWOvsLcoQi2kpHIA; s_ecid=MCMID%7C51499273735922173403879288947271341352; AMCVS_F6301253512D2BDB0A490D45%40AdobeOrg=1; dtCookie=v_4_srv_5_sn_UKGS61LHKE95F58CKCJ5JTTUHNJV2N7D_app-3A1b02c17e3de73d2a_1_ol_0_perc_100000_mul_1; _cs_mk=0.5500628905410729_1697989446664; s_cc=true; AMCV_F6301253512D2BDB0A490D45%40AdobeOrg=1585540135%7CMCMID%7C51499273735922173403879288947271341352%7CMCAID%7CNONE%7CMCOPTOUT-1697996646s%7CNONE%7CMCAAMLH-1698594246%7C3%7CMCAAMB-1698594246%7Cj8Odv6LonN4r3an7LhD3WZrU1bUpAkFkkiY1ncBR96t2PTI%7CMCCIDH%7C1907712470%7CvVersion%7C4.4.0; aam_uuid=56460070521806806704392296716542884874; locDestZip=96939; locStoreId=1760; sc-location-v2=%7B%22meta%22%3A%7B%22CreatedAt%22%3A%222023-10-22T15%3A44%3A06.975Z%22%2C%22ModifiedAt%22%3A%222023-10-22T15%3A44%3A07.381Z%22%2C%22ExpiresAt%22%3A%222024-10-21T15%3A44%3A07.381Z%22%7D%2C%22value%22%3A%22%7B%5C%22physical%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%2C%5C%22source%5C%22%3A%5C%22G%5C%22%2C%5C%22captureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A06.975Z%5C%22%7D%2C%5C%22destination%5C%22%3A%7B%5C%22zipCode%5C%22%3A%5C%2296939%5C%22%7D%2C%5C%22store%5C%22%3A%7B%5C%22storeId%5C%22%3A1760%2C%5C%22zipCode%5C%22%3A%5C%2299504%5C%22%2C%5C%22storeHydratedCaptureTime%5C%22%3A%5C%222023-10-22T15%3A44%3A07.380Z%5C%22%7D%7D%22%7D; __gsas=ID=43dc00dcffeab34e:T=1697989465:RT=1697989465:S=ALNI_MYLHkniZY8kqCiAFOeNu1jnR4mz0w; dtSa=-; cto_bundle=2D7FnF9ZMHJPQlFCbkdTMktUSFREZ2pVJTJGajJMRFFsd2lINnRNRkZxY0dFU1lqJTJCN0glMkZMU0FqRTR0UyUyRmZRa1FscDdyV0tQUTNZdzVBM1g2WkJHUENTUEdlaGtUdWtiZWU4allOYlc2dyUyRm1VeiUyRlVBZVZkdVRmSFElMkJZQ0ExRk9mZzZNV1VNd1ZYSXZ5RWZSeUFQdkJXZ3VxZzZJZyUzRCUzRA; blue-assist-banner-shown=true; _cs_c=1; _gcl_au=1.1.1372174147.1697989479; dtLatC=1; _abck=2025C1ED2DAE1BA19B91708C91F51C0F~0~YAAQHLQRYGhMakWLAQAAjTYLWgqyxn7G2wIoFoVC+4nrsT1cxJIaO1O5ytS58DrifnksxvYxu7oOIuZmBDszkeEGLUk/7ekIvtGFO7u2yogmIcW17juPvPSDc1XdGYIVbijt6PbXvKVWeAB+8ZIF6voDPAwIN8H+QKpGl7va06mSquCsIXDORvQ1fz6MaHlKajkG/g9N8gGFlrsBxnMpRA0vk4b7Xv9obYx0wvld8KvntBNHHmpIs0djlSe17djNQz57X3JJHstt9/StCh7Jo00MTiV93eKEGVBoMzoq4+PxnTdsrKg5PkI1bneUzJMSGuV43ZaXWfbm7uJ5sVfxdvHl0uQOQUh7ClSLpjFxe7sR9F6ZRsJ1uTIjK2Ab7WfvjLZd5C8V7/qZhg/oMP3pF0Dt09LThXO7tonFOvt8UhAETsU0Hw6+K/m4mS0wH46V+5rfa+qmNcM=~-1~-1~-1; bm_sz=DC447A131B862AC781959292B401C641~YAAQHLQRYGlMakWLAQAAjTYLWhWe27kjreKQmsKd+a2iqr9yFDHU3maKKvHTexZicnoFjIsx0OiZ03lAbfGOl2IZo7UNsbeBjNT3emSu3sSR0HUl0ddFd8LjnFGqQISSIw7upSTqhbE/Ccdgbo842X0fWkxXLQCXe7eIC5cgVWU1GMRdWc34I/WgCiVwaRV2v6j3I93rIKuMA5dYvCv2yQykBKCPPN4sbyl8TEvfZ+XgvWuziGVpb4G+3OBohzrz8/j7ZnhXQ1U0WZARKye28p1zLuSDfDk4mInPZlvumI5oeG13Z+CjpYEKf7D5iAjzcWRGlsQ32gejCk7aPI6RC1dkVBh/DL00bGUor4wdKjruVwQNpz0v3hop17nvb4BKkQIqQAfEL6zMaGHLj9ycBq93U+2b2AXxNlcKMzEYuQ0cL/PuJIlwGBiqjQ==~4601667~3422276; dtPC=5$589618119_27h-vVFAAHBSMMCTLEHFRWVQLIOPDRRSURPFR-0e0; rxvt=1698031868366|1698030068366; _cs_id=3fe9d270-9876-ad3a-cae4-0084c344a27c.1697989478.5.1698030169.1698030169.1645469968.1732153478774; _cs_s=1.0.0.1698031970357; c2=pdp%3A%20ratingsreviews',  # Replace with the actual Cookie\n",
    "        'Downlink': '10',\n",
    "        'Dpr': '1',\n",
    "        'Referer': url,\n",
    "        'Sec-Ch-Ua': '\"Google Chrome\";v=\"117\", \"Not;A=Brand\";v=\"8\", \"Chromium\";v=\"117\"',\n",
    "        'Sec-Ch-Ua-Mobile': '?0',\n",
    "        'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup \n",
    "\n",
    "def bestbuy_review(soup, url):    \n",
    "    bestbuy = {}\n",
    "    bestbuy_reviews = []   \n",
    "    Model =  soup.find(\"h2\", {\"class\": \"heading-6 product-title mb-100\"}).text\n",
    "    Review_average = soup.find(\"span\", {\"class\": \"ugc-c-review-average font-weight-medium order-1\"}).text\n",
    "        \n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"review-item-content col-xs-12 col-md-9\"})\n",
    "    for item in review_session:    \n",
    "        bestbuy = {    \n",
    "             'Model': Model,\n",
    "             'Review average': Review_average,\n",
    "             'Review date': item.find(\"div\", {\"class\": \"posted-date-ownership disclaimer v-m-right-xxs\"}).text.replace('Posted','')  \n",
    "            ,'URL':url \n",
    "        }\n",
    "        \n",
    "        try:    \n",
    "            bestbuy['Review title']  = item.find(\"h4\", {\"class\": \"c-section-title review-title heading-5 v-fw-medium\"}).text  \n",
    "        except AttributeError:    \n",
    "            bestbuy['Review title']  = None\n",
    "        # print(bestbuy['Review title'])    \n",
    "        try:    \n",
    "            bestbuy['Review rating']  = item.find(\"div\", {\"class\": \"c-ratings-reviews flex c-ratings-reviews-small align-items-center gap-50\"}).text.replace(' out of 5 stars','').replace('Rated ','')  \n",
    "        except AttributeError:    \n",
    "            bestbuy['Review rating']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     bestbuy['Review promotion']  = item.find(\"div\", {\"class\": \"body-copy-sm pt-50\"}).text\n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['Review promotion']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     bestbuy['Review aggregation']  = item.find(\"p\", {\"class\": \"body-copy ugc-related-product\"}).text\n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['Review aggregation']  = None\n",
    "            \n",
    "        try:    \n",
    "            bestbuy['Review content']  = item.find(\"div\", {\"class\": \"ugc-review-body\"}).text  \n",
    "        except AttributeError:    \n",
    "            bestbuy['Review content']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     bestbuy['Review Recommendation']  = item.find(\"div\", {\"class\": \"ugc-recommendation\"}).text  \n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['Review Recommendation']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     network_badge  = item.find(\"div\", {\"class\": \"ugc-network-badge\"})\n",
    "        #     if network_badge:\n",
    "        #         bestbuy['Seeding or not'] = network_badge.get(\"data-track\")\n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['Seeding or not']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     bestbuy['People find helpful']  = item.find(\"button\", {\"data-track\": \"Helpful\"}).text\n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['People find helpful']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     bestbuy['People find unhelpful']  = item.find(\"button\", {\"data-track\": \"Unhelpful\"}).text\n",
    "        # except AttributeError:    \n",
    "        #     bestbuy['People find unhelpful']  = None\n",
    "\n",
    "  \n",
    "        bestbuy_reviews.append(bestbuy)    \n",
    "    \n",
    "  \n",
    "    return bestbuy_reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f8750-55bd-4732-bedf-a324c3aac49d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c9febb-207c-48b9-bec4-8e6ee58b66d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.bestbuy.com/site/reviews/hp-936-standard-capacity-ink-cartridge-cyan/6562271?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-936-standard-capacity-ink-cartridge-black/6562273?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-936-4-pack-standard-capacity-ink-cartridges-black-magenta-yellow-cyan/6562274?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923-standard-capacity-ink-cartridge-cyan/6562093?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923-standard-capacity-ink-cartridge-magenta/6562092?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923-standard-capacity-ink-cartridge-yellow/6562094?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923-standard-capacity-ink-cartridge-black/6562091?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923-4-pack-standard-capacity-ink-cartridges-black-magenta-yellow-cyan/6562090?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923e-evomore-ink-cartridge-cyan/6578588?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923e-evomore-ink-cartridge-magenta/6578586?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923e-evomore-ink-cartridge-yellow/6578585?variant=A',\n",
       " 'https://www.bestbuy.com/site/reviews/hp-923e-evomore-ink-cartridge-black/6578589?variant=A']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"Review text scrape URL.xlsx\"\n",
    "sheets = 'Bestbuy'\n",
    "bestbuy_url = pd.read_excel(path, sheet_name = sheets)\n",
    "urls = bestbuy_url['URL'].to_list()\n",
    "url_list = []\n",
    "for value in urls:\n",
    "    if pd.notna(value):\n",
    "        url_list.append(value)\n",
    "url_list\n",
    "# url_list = ['https://www.bestbuy.com/site/reviews/hp-936-standard-capacity-ink-cartridge-cyan/6562271?variant=A',\n",
    "#  'https://www.bestbuy.com/site/reviews/hp-936-standard-capacity-ink-cartridge-black/6562273?variant=A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e9310-1f90-45be-a834-48278c7f3bac",
   "metadata": {},
   "source": [
    "## Scrape Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestbuy_reviews = []    \n",
    "  \n",
    "for link in url_list:    \n",
    "    print(link)  \n",
    "    # print(\"Total review\",len(bestbuy_reviews))    \n",
    "    should_continue = True  \n",
    "    for x in range(1, 1000):  \n",
    "        if not should_continue:  \n",
    "            break   \n",
    "        while True:\n",
    "            try:\n",
    "                url = f'{link}&page={x}'  \n",
    "                soup = get_review_bestbuy(url)  \n",
    "                # print(f'Getting page: {x}') \n",
    "                reviews = bestbuy_review(soup, url)  \n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')    \n",
    "\n",
    "                bestbuy_reviews.extend(reviews)    \n",
    "                \n",
    "                next_page_link = soup.find(\"li\", {\"class\": \"page next\"})  # Note: Use lowercase \"true\" for attribute value\n",
    "                # print(next_page_link)\n",
    "                if next_page_link is None:\n",
    "                    should_continue = False\n",
    "                    # print('No more pages left')\n",
    "                    break \n",
    "                else:   \n",
    "                    break\n",
    "        \n",
    "            except Exception as e:  \n",
    "                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")  \n",
    "                        time.sleep(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6495c4-3821-4c0d-904d-7f4dbad4dd1a",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "bestbuy_review = pd.DataFrame(bestbuy_reviews)\n",
    "bestbuy_review['Retailer']=\"bestbuy\"\n",
    "bestbuy_review['Scraping Date'] = date.today().strftime('%Y-%m-%d')\n",
    "bestbuy_review['HP Model Number'] = bestbuy_review['Model'].str.extract(r'(\\d+e*)')\n",
    "# bestbuy_review['People find helpful'] = bestbuy_review['People find helpful'].str.extract(r'(\\d+)').astype(int)\n",
    "# bestbuy_review['People find unhelpful'] = bestbuy_review['People find unhelpful'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "new_column_order = ['Retailer', 'HP Model Number', 'Model', 'Review date', 'Review title', 'Review rating', 'Review content', 'Scraping Date', 'URL']\n",
    "bestbuy_review = bestbuy_review[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9403bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.today().strftime('%Y%m%d')  \n",
    "bestbuy_review.to_excel(f'scraped_reviews/bestbuy_reviews_{date}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3b7ce-e0a5-43ee-9a10-59a8d7964c3a",
   "metadata": {},
   "source": [
    "# Staples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a78161-e808-4cda-b1f1-0a3339400531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_staple(url):   \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, 'html.parser') \n",
    "    \n",
    "    driver.quit()\n",
    "    return soup  \n",
    "  \n",
    "def staple_review(soup, url):    \n",
    "    staple = {}\n",
    "    staple_reviews = []   \n",
    "    try:\n",
    "        model = soup.find(\"a\", attrs={\"class\": \"list-tile__product_title\"}).string.strip()   \n",
    "    except AttributeError: \n",
    "        model = soup.title.text.replace(\"Staples Customer Reviews for \",\"\")   \n",
    "    try: \n",
    "        review_average = soup.find(\"div\", attrs={\"class\": \"AverageRating__ratingCircleSection\"}).text\n",
    "    except AttributeError:\n",
    "        review_average = soup.find(\"div\", attrs={\"class\": \"sc-14dq9fm-4 chBfYj\"}).text\n",
    "        \n",
    "    review_session = soup.find_all(\"div\", {\"class\": \"ReviewList__reviewSection\"})\n",
    "    \n",
    "    \n",
    "    for item in review_session:    \n",
    "        staple = {    \n",
    "            'Model': model,    \n",
    "            'Review average' : review_average,\n",
    "            'Review date': item.find(\"div\", {\"class\": \"ReviewDate__reviewDate\"}).text.replace('Posted',''),     \n",
    "            \"Review content\": item.find(\"div\", {\"class\": \"ReviewDescription__reviewDescription\"}).text,  \n",
    "            \"URL\" : url  \n",
    "        }\n",
    "        \n",
    "        try:    \n",
    "            staple['Review title']  = item.find(\"div\", {\"class\": \"ReviewTitle__reviewTitle\"}).text  \n",
    "        except AttributeError:    \n",
    "            staple['Review title']  = None\n",
    "        # print(staple['Review title'])\n",
    "            \n",
    "        # try:    \n",
    "        #     staple['Response name']  = item.find(\"div\", {\"class\": \"ReviewResponse__responseUser\"}).text  \n",
    "        # except AttributeError:    \n",
    "        #     staple['Response name']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     staple['Response text']  = item.find(\"div\", {\"class\": \"ReviewResponse__responseText\"}).text  \n",
    "        # except AttributeError:    \n",
    "        #     staple['Response text']  = None\n",
    "            \n",
    "        # try:    \n",
    "        #     staple['Response date']  = item.find(\"div\", {\"class\": \"ReviewResponse__responderDetail\"}).text  \n",
    "        # except AttributeError:    \n",
    "        #     staple['Response date']  = None\n",
    "            \n",
    "        # try:\n",
    "        #     staple['Seeding or not'] = item.find(\"div\", {\"class\":\"ReviewBadge__incentivized\"}).text\n",
    "        # except AttributeError:\n",
    "        #     staple['Seeding or not'] = None\n",
    "        \n",
    "        # try:    \n",
    "        #     staple[\"Verified Purchase or not\"] = item.find(\"div\", {\"class\": \"ReviewBadge__badge\"}).text  \n",
    "        # except AttributeError:    \n",
    "        #     staple[\"Verified Purchase or not\"] = None    \n",
    "  \n",
    "        # try:      \n",
    "        #     staple[\"Review name\"] = item.find(\"div\", {\"class\": \"ReviewUser__reviewUser\"}).text  \n",
    "        # except AttributeError:        \n",
    "        #     staple[\"Review name\"] = None  \n",
    "  \n",
    "        # try:    \n",
    "        #     staple[\"People find helpful\"] = item.find(\"span\", {\"class\": \"ThumbsToggle__sr_only\"}).text.replace('up votes ','')  \n",
    "        # except AttributeError:    \n",
    "        #     staple[\"People find helpful\"] = None  \n",
    "            \n",
    "        # try:    \n",
    "        #     staple[\"People find unhelpful\"] = item.find(\"span\", {\"class\": \"ThumbsToggle__sr_only\"}).text.replace('down votes ','')  \n",
    "        # except AttributeError:    \n",
    "        #     staple[\"People find unhelpful\"] = None  \n",
    "            \n",
    "        try:\n",
    "            staple['Syndicated source'] = item.find(\"div\", {\"class\": \"ReviewBadge__reviewedAtText\"}).string\n",
    "        except AttributeError:   \n",
    "            staple['Syndicated source'] = None \n",
    "            \n",
    "        review_star_sections = item.find_all(\"div\", {\"class\": \"ReviewRow__reviewStarSection\"})\n",
    "        for star in review_star_sections:  \n",
    "            rating_div = star.find(\"div\", {\"class\": re.compile(r'\\d_star')})  \n",
    "        if rating_div:  \n",
    "            staple[\"Review rating\"] = re.search(r'(\\d)_star', rating_div['class'][0]).group(1)  \n",
    "\n",
    "  \n",
    "        staple_reviews.append(staple)    \n",
    "    \n",
    "  \n",
    "    return staple_reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa47d8e-623f-4510-9836-5ba5318dc68e",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec2eb7c-236f-4608-adef-fe15cba8cf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.staples.com/ptd/review/24582881',\n",
       " 'https://www.staples.com/ptd/review/24582882',\n",
       " 'https://www.staples.com/ptd/review/24582883',\n",
       " 'https://www.staples.com/ptd/review/24582884',\n",
       " 'https://www.staples.com/ptd/review/24582885',\n",
       " 'https://www.staples.com/ptd/review/24582886',\n",
       " 'https://www.staples.com/ptd/review/24582887',\n",
       " 'https://www.staples.com/ptd/review/24582888',\n",
       " 'https://www.staples.com/ptd/review/24582889',\n",
       " 'https://www.staples.com/ptd/review/24582890',\n",
       " 'https://www.staples.com/ptd/review/24590160',\n",
       " 'https://www.staples.com/ptd/review/24590161',\n",
       " 'https://www.staples.com/ptd/review/24590162',\n",
       " 'https://www.staples.com/ptd/review/24590163',\n",
       " 'https://www.staples.com/ptd/review/24590164',\n",
       " 'https://www.staples.com/ptd/review/24590165',\n",
       " 'https://www.staples.com/ptd/review/24590166',\n",
       " 'https://www.staples.com/ptd/review/24590167',\n",
       " 'https://www.staples.com/ptd/review/24599304',\n",
       " 'https://www.staples.com/ptd/review/24599307',\n",
       " 'https://www.staples.com/ptd/review/24599311',\n",
       " 'https://www.staples.com/ptd/review/24599312',\n",
       " 'https://www.staples.com/ptd/review/24599313',\n",
       " 'https://www.staples.com/ptd/review/24599314']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =  r\"Review text scrape URL.xlsx\"\n",
    "sheets = 'Staples'\n",
    "staple_url = pd.read_excel(path, sheet_name = sheets)\n",
    "urls = staple_url['URL'].to_list()\n",
    "\n",
    "url_list = []\n",
    "for value in urls:\n",
    "    if pd.notna(value):\n",
    "        url_list.append(value)\n",
    "url_list\n",
    "# url_list = ['https://www.staples.com/ptd/review/24590167']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a495ed-0f95-45e9-878d-0e481b8a175d",
   "metadata": {},
   "source": [
    "## Scrape Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de145b0c-a38f-4b1e-9bcb-c1258db7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "staple_reviews = []    \n",
    "  \n",
    "for link in url_list:    \n",
    "    print(link)  \n",
    "    should_continue = True  \n",
    "    for x in range(1, 500):  \n",
    "        if not should_continue:  \n",
    "            break   \n",
    "        while True:\n",
    "            try:\n",
    "                url = f'{link}/?page={x}'  \n",
    "                soup = get_soup_staple(url)  \n",
    "                # print(f'Getting page: {x}')    \n",
    "                reviews = staple_review(soup, url)   \n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')    \n",
    "\n",
    "                staple_reviews.extend(reviews)    \n",
    "#                 print(\"Total review\",len(staple_reviews))    \n",
    "\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-label\": \"Next page of results\"})       \n",
    "                # print(next_page_link)\n",
    "                if next_page_link is not None:\n",
    "                    tabindex = next_page_link.get(\"tabindex\")      \n",
    "                    if tabindex == \"-1\": \n",
    "                        should_continue = False \n",
    "                        print('No more pages left')      \n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "                else: \n",
    "                    should_continue = False \n",
    "                    break\n",
    "        \n",
    "            except Exception as e:  \n",
    "                        print(f\"Error encountered: {e}. Retrying in 3 seconds...\")  \n",
    "                        time.sleep(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc6b15-0ec2-48da-bcc6-ada205fe104e",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4e862-6dff-49f5-a5de-df98959b936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime  \n",
    "pd.set_option('display.max_columns', None)\n",
    "staple = pd.DataFrame(staple_reviews)\n",
    "staple\n",
    "staple['Retailer']=\"Staples\"\n",
    "# staple['Scraping Date'] = date.today().strftime('%Y-%m-%d')\n",
    "staple['HP Model Number'] = staple['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "# staple['Review date'] = pd.to_datetime(staple['Review date'])\n",
    "staple['Review date'] = pd.to_datetime(staple['Review date'].str.strip(), format='%b %d, %Y')\n",
    "# staple['People find helpful'] = staple['People find helpful'].str.extract(r'(\\d+)').astype(int)\n",
    "# staple['People find unhelpful'] = staple['People find unhelpful'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "new_column_order = ['Retailer', 'HP Model Number', 'Model', 'Review date', 'Review title', 'Review rating', 'Review content', \n",
    "                    'Syndicated source', 'URL']\n",
    "staple = staple[new_column_order]\n",
    "\n",
    "date = datetime.today().strftime('%Y%m%d')  \n",
    "staple.to_excel(f'scraped_reviews/staple_reviews_{date}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01be937-e269-446e-91be-ca95a8542fc5",
   "metadata": {},
   "source": [
    "# Walmart \n",
    "## Run either method 1 or 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac6b04-4a47-4f0d-979b-fef79a63b572",
   "metadata": {},
   "source": [
    "## Method 1: Use API\n",
    "Apply for API: https://api.scrapingdog.com/login (need to register account, one account has 1K times for scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7afacea-f304-4703-ab68-86c72b3f7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_walmart(url, max_retries=5):  \n",
    "    extracted_reviews = []  \n",
    "    retry_count = 0  \n",
    "\n",
    "    # headers = {\n",
    "    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    # 'Accept-Language': 'en-US,en;q=0.5',\n",
    "    # 'Referer': 'https://www.google.com/',\n",
    "    # }\n",
    "    while retry_count < max_retries:  \n",
    "        response = requests.get(url)  \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "        # html_content = response.text   \n",
    "        return soup\n",
    "    # while retry_count < max_retries:  \n",
    "    #     try:\n",
    "    #         response = requests.get(url, headers=headers)  \n",
    "    #         if response.status_code == 200:\n",
    "    #             soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "    #             return soup\n",
    "    #         else:\n",
    "    #             print(f\"Error: Received response code {response.status_code}\")\n",
    "    #     # html_content = response.text   \n",
    "    #     # print(response)\n",
    "    #     except requests.RequestException as e:\n",
    "    #         print(f\"Request failed: {e}\")\n",
    "    # return None\n",
    "\n",
    "def walmart_review(soup, url):    \n",
    "    parsed_url = urlparse(url)\n",
    "    url_param = parse_qs(parsed_url.query).get('url', [''])[0]\n",
    "    walmart = {}  \n",
    "    walmart_reviews = []  \n",
    "    try:\n",
    "        Model = soup.find(\"a\", {\"class\": \"w_x7ug f6 dark-gray\"}).text.replace('Back to ', '').strip()\n",
    "    except AttributeError:\n",
    "        Model = None\n",
    "    # print(Model)\n",
    "    try:\n",
    "        Review_average = soup.find(\"span\", {\"class\": \"f-headline b\"}).text.replace(' out of 5', '').strip()\n",
    "    except AttributeError:\n",
    "        Review_average = None\n",
    "    # print(Review_average)\n",
    "    \n",
    "    review_section = soup.find_all(\"li\", {\"class\": \"dib w-100 mb3\"})\n",
    "    \n",
    "    for item in review_section:    \n",
    "        walmart = {    \n",
    "            'Model': Model,    \n",
    "            'Review average' : Review_average,\n",
    "            # 'Review date': item.find(\"div\", {\"class\": \"f7 gray mt1\"}).text,     \n",
    "            # \"Review Content\": item.find(\"div\", {\"class\": \"f6 mid-gray lh-copy\"}).text,  \n",
    "            \"URL\" : url\n",
    "        }\n",
    "        try:    \n",
    "            walmart['Review title']  = item.find(\"h3\", {\"class\": \"w_kV33 w_Sl3f w_mvVb f5 b\"}).text  \n",
    "\n",
    "        except AttributeError:    \n",
    "            walmart['Review title']  = None\n",
    "        print(walmart['Review title'])\n",
    "        \n",
    "        try:    \n",
    "            walmart['Review rating']  = item.find(\"span\", {\"class\": \"w_iUH7\"}).text.replace(' out of 5 stars review','')\n",
    "\n",
    "        except AttributeError:    \n",
    "            try:\n",
    "                walmart['Review rating']  = item.find(\"div\", {\"class\": \"flex flex-grow-1\"}).text.replace(' out of 5 stars review','')\n",
    "            except:\n",
    "                walmart['Review rating']  = None\n",
    "        print(walmart['Review rating'])\n",
    "        try: \n",
    "            walmart['Review content'] = item.find(\"span\", {\"class\": \"tl-m mb3 db-m\"}).text\n",
    "        except AttributeError:\n",
    "            walmart['Review content'] = None\n",
    "\n",
    "        try: \n",
    "            walmart['Review date'] = item.find(\"div\", {\"class\": \"f7 gray mt1\"}).text\n",
    "        except AttributeError:\n",
    "            walmart['Review date'] = None\n",
    "            \n",
    "        # try:\n",
    "        #     walmart['Incentivized review'] = item.find(\"span\", {\"class\": \"w_VbBP w_0FMU w_I_19 mb2 mr2\"}).text \n",
    "        # except AttributeError:    \n",
    "        #     walmart['Incentivized review']  = None\n",
    "\n",
    "        # try:\n",
    "        #     walmart['Verified Purchase or not'] = item.find(\"span\", {\"class\": \"green b mr1\"}).text \n",
    "        # except AttributeError:    \n",
    "        #     try: \n",
    "        #         walmart['Verified Purchase or not'] = item.find(\"div\", {\"class\": \"pl2 f7 self-center\"}).text\n",
    "        #     except:\n",
    "        #         walmart['Verified Purchase or not']  = None\n",
    "        walmart_reviews.append(walmart)   \n",
    "    print(walmart_reviews)\n",
    "    return walmart_reviews \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee6c6a-99c4-461c-9508-f2f8c81e961c",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bfe8fa-448a-4c10-b34f-07b9a4c7d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"Review text scrape URL.xlsx\"\n",
    "sheets = \"Walmart\"\n",
    "url = pd.read_excel(path, sheet_name = sheets)\n",
    "all_list = url['URL'].to_list()\n",
    "link_list = []\n",
    "for value in all_list:\n",
    "    if pd.notna(value):\n",
    "        link_list.append(value)\n",
    "link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0bfcb-e06a-4432-aa3e-56a12a233955",
   "metadata": {},
   "source": [
    "## Scrape Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ab00f-c714-4507-afc5-a0b56271ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '665ed06172077d78c90a593c' # Replace with your api key\n",
    "\n",
    "walmart_reviews = []    \n",
    "    \n",
    "for link in link_list:    \n",
    "    print(link)  \n",
    "    should_continue = True \n",
    "    for x in range(1, 500):  \n",
    "        if not should_continue:  \n",
    "            break   \n",
    "        while True:\n",
    "            try:\n",
    "                url = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={link}/?page={x}\"\n",
    "                # url = f\"https://api.scrapingdog.com/scrape?api_key={api_key}&url={link}/?page=3\"\n",
    "                soup = get_soup_walmart(url)\n",
    "                # print(soup)\n",
    "                # print('yes')\n",
    "                reviews = walmart_review(soup, url)   \n",
    "                # print('no')\n",
    "                # print(reviews)\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')  \n",
    "\n",
    "                walmart_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-label\": \"Next Page\"})       \n",
    "                # print(next_page_link)\n",
    "\n",
    "                if next_page_link is None:\n",
    "                    should_continue = False \n",
    "                    # print('No more pages left')      \n",
    "                    break  \n",
    "                else:   \n",
    "                    break\n",
    "\n",
    "            except Exception:  \n",
    "                print(f\"Error encountered. Retrying in 3 seconds...\")  \n",
    "                time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f846d1-7279-44df-ada5-868297d37944",
   "metadata": {},
   "source": [
    "## Method 2: Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f9da73-8736-456e-a013-633ebb157a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):   \n",
    "    # r = requests.get('http://localhost:8050/render.html', params={'url': url, 'wait': 2})  \n",
    "    # soup = BeautifulSoup(r.text, 'html.parser')  \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, 'html.parser')     \n",
    "    driver.quit()\n",
    "    # print(soup)\n",
    "    return soup  \n",
    "\n",
    "def walmart_review(soup, url):    \n",
    "    walmart = {}  \n",
    "    walmart_reviews = []  \n",
    "    \n",
    "    Model = soup.find(\"a\", {\"class\": \"w_x7ug f6 dark-gray\"}).text.replace('Back to ', '').strip()\n",
    "    # print(\"Model: \", Model) \n",
    "    Review_average = soup.find(\"span\", {\"class\": \"f-headline b\"}).text.replace(' out of 5', '')\n",
    "    # print(Review_average)\n",
    "    review_section = soup.find_all(\"li\", {\"class\": \"dib w-100 mb3\"})\n",
    "    # print(review_section)\n",
    "    for item in review_section:    \n",
    "        walmart = {    \n",
    "            'Model': Model,    \n",
    "            'Review average' : Review_average,  \n",
    "            # \"Review Content\": item.find(\"div\", {\"class\": \"f6 mid-gray lh-copy\"}).text,  \n",
    "            \"URL\" : url\n",
    "        }\n",
    "        try:    \n",
    "            walmart['Review title']  = item.find(\"h3\", {\"class\": \"w_kV33 w_Sl3f w_mvVb f5 b\"}).text  \n",
    "        except AttributeError:    \n",
    "            walmart['Review title']  = None\n",
    "        # print(walmart['Review title'])\n",
    "        \n",
    "        try:    \n",
    "            walmart['Review rating']  = item.find(\"span\", {\"class\": \"w_iUH7\"}).text.replace(' out of 5 stars review','')\n",
    "        except AttributeError:    \n",
    "            try:\n",
    "                walmart['Review rating']  = item.find(\"div\", {\"class\": \"flex flex-grow-1\"}).text.replace(' out of 5 stars review','')\n",
    "            except:\n",
    "                walmart['Review rating']  = None\n",
    "        # print(walmart['Review rating'])\n",
    "        try: \n",
    "            walmart['Review content'] = item.find(\"span\", {\"class\": \"tl-m mb3 db-m\"}).text\n",
    "        except AttributeError:\n",
    "            walmart['Review content'] = None\n",
    "        try: \n",
    "            walmart['Review date'] = item.find(\"div\", {\"class\": \"f7 gray mt1\"}).text\n",
    "        except AttributeError:\n",
    "            walmart['Review date'] = None\n",
    "            \n",
    "        # try:\n",
    "        #     walmart['Incentivized review'] = item.find(\"span\", {\"class\": \"w_VbBP w_0FMU w_I_19 mb2 mr2\"}).text \n",
    "        # except AttributeError:    \n",
    "        #     walmart['Incentivized review']  = None\n",
    "\n",
    "        # try:\n",
    "        #     walmart['Verified Purchase or not'] = item.find(\"span\", {\"class\": \"green b mr1\"}).text \n",
    "        # except AttributeError:    \n",
    "        #     try: \n",
    "        #         walmart['Verified Purchase or not'] = item.find(\"div\", {\"class\": \"pl2 f7 self-center\"}).text\n",
    "        #     except:\n",
    "        #         walmart['Verified Purchase or not']  = None\n",
    "\n",
    "        walmart_reviews.append(walmart)    \n",
    "    # print(walmart_reviews)\n",
    "    return walmart_reviews \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd8479-1506-4fbf-aeeb-8eeb1c50aa4d",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f817b82b-1847-4a62-addc-3fa3a2e1f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"Review text scrape URL.xlsx\"\n",
    "sheets = \"Walmart\"\n",
    "url = pd.read_excel(path, sheet_name = sheets)\n",
    "all_list = url['URL'].to_list()\n",
    "link_list = []\n",
    "for value in all_list:\n",
    "    if pd.notna(value):\n",
    "        link_list.append(value)\n",
    "link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74750c-3e77-4874-98b8-dc072f31960a",
   "metadata": {},
   "source": [
    "## Scrape Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22986b-8809-42fd-97ac-6484b5bc5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_reviews = []    \n",
    "    \n",
    "for link in link_list:    \n",
    "    print(link)  \n",
    "    should_continue = True \n",
    "    for x in range(1, 500):  \n",
    "        if not should_continue:  \n",
    "            break   \n",
    "        while True:\n",
    "            try:\n",
    "                url = f\"{link}?page={x}\"\n",
    "                soup = get_soup(url)  \n",
    "                # print(soup)\n",
    "                reviews = walmart_review(soup, url)   \n",
    "                print(reviews)\n",
    "\n",
    "                print(f'Extracted reviews on page {x}: {len(reviews)}')  \n",
    "\n",
    "                walmart_reviews.extend(reviews)\n",
    "\n",
    "                next_page_link = soup.find(\"a\", {\"aria-label\": \"Next Page\"})       \n",
    "                # print(next_page_link)\n",
    "\n",
    "                if next_page_link is None:\n",
    "                    should_continue = False \n",
    "                    # print('No more pages left')      \n",
    "                    break  \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            except Exception:  \n",
    "                print(f\"Error encountered. Retrying in 3 seconds...\")  \n",
    "                time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b80308-6cc0-4986-89be-c7c5083b5d88",
   "metadata": {},
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa70e3b-3f4d-412d-825f-9313581a2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date  \n",
    "pd.set_option('display.max_columns', None)\n",
    "walmart = pd.DataFrame(walmart_reviews)\n",
    "walmart['Retailer']= \"Walmart\"\n",
    "walmart['HP Model Number'] = walmart['Model'].str.extract(r'(\\d+[e]*)', expand=False)\n",
    "walmart['Scraping Date'] = date.today().strftime('%Y-%m-%d')\n",
    "walmart['Review date'] = pd.to_datetime(walmart['Review date'].str.strip(), format='%m/%d/%Y')\n",
    "\n",
    "new_column_order = ['Retailer', 'HP Model Number', 'Model', 'Review date', 'Review title', 'Review rating', 'Review content', 'URL']\n",
    "\n",
    "walmart = walmart[new_column_order]\n",
    "\n",
    "date = datetime.today().strftime('%Y%m%d')  \n",
    "walmart.to_excel(f'scraped_reviews/walmart_reviews_{date}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826ded0-e7d6-40b1-98e2-7f08ff99b9f9",
   "metadata": {},
   "source": [
    "## HP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fd782-01c2-4b3b-a283-3b9494a30915",
   "metadata": {},
   "source": [
    "### Products are scraped using SKU/Product ID by Bazaarvoice API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c916f77a-c983-4774-a41f-4a44be681c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "# Import product number that needs to scrape from excel \n",
    "path = r'Review text scrape URL.xlsx'\n",
    "sheet = 'HP'\n",
    "hp = pd.read_excel(path, sheet_name=sheet)\n",
    "skus = hp['SKU'].to_list()\n",
    "\n",
    "sku_list = []\n",
    "for value in skus:\n",
    "    if pd.notna(value):\n",
    "        sku_list.append(value)\n",
    "sku_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cc447a-64e3-4959-b889-ba96ad22c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_review(sku):\n",
    "    \n",
    "    api_url = 'https://api.bazaarvoice.com/data/reviews.json'\n",
    "\n",
    "#Input own pass key\n",
    "    params = {\n",
    "        'resource': 'reviews',\n",
    "        'action': 'REVIEWS_N_STATS',\n",
    "        'filter': f'productid:eq:{sku}',\n",
    "        'include': 'authors,products,comments',\n",
    "        'limit': 100,\n",
    "        'offset': 0,\n",
    "        'sort': 'submissiontime:desc',\n",
    "        'passkey': '',\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '8843-en_us'\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    # if response.status_code != 200:\n",
    "    #     print(f'Error: status code {response.status_code}')\n",
    "    #     return pd.DataFrame()\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "\n",
    "    limit = 100\n",
    "    no_batch = math.ceil(data['TotalResults']/limit)\n",
    "    print('Total review',data['TotalResults'])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for x in range(0, no_batch):\n",
    "        #Input own pass key\n",
    "        offset = x*limit\n",
    "#         print('Offset',offset)\n",
    "        params = {\n",
    "        'resource': 'reviews',\n",
    "        'action': 'REVIEWS_N_STATS',\n",
    "        'filter': f'productid:eq:{sku}',\n",
    "        'include': 'authors,products,comments',\n",
    "        'limit': 100,\n",
    "        'offset': offset,\n",
    "        'sort': 'submissiontime:desc',\n",
    "        'passkey': '',\n",
    "        'apiversion': '5.5',\n",
    "        'displaycode': '8843-en_us'\n",
    "        }\n",
    "\n",
    "        response = requests.get(api_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = json.loads(response.text)\n",
    "        else:\n",
    "            print('status code != 200')\n",
    "\n",
    "        results_data = data['Results']\n",
    "        df_temp = pd.DataFrame(results_data)\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "        time.sleep(3)\n",
    "    if not df.empty:\n",
    "        df['SyndicationSource_Name'] = df.apply(lambda row: row['SyndicationSource'].get('Name') if row['IsSyndicated'] else None, axis=1)\n",
    "        required_columns = ['OriginalProductName', 'SubmissionTime', 'Title', 'Rating', 'ReviewText', 'SyndicationSource_Name']\n",
    "        available_columns = [col for col in required_columns if col in df.columns]\n",
    "        df = df[available_columns]\n",
    "\n",
    "        # Rename the columns\n",
    "        new_column_names = {\n",
    "            'OriginalProductName': 'Model',\n",
    "            'Rating': 'Review rating',\n",
    "            'SubmissionTime': 'Review date',\n",
    "            'Title': 'Review title',\n",
    "            'ReviewText': 'Review content',\n",
    "            'SyndicationSource_Name': 'Syndicated source'\n",
    "        }\n",
    "        df = df.rename(columns=new_column_names)\n",
    "        df['HP Model Number'] = df['Model'].str.extract(r'(\\d+e?)')\n",
    "\n",
    "        cols = df.columns.tolist()\n",
    "        cols = ['HP Model Number'] + cols[:-1]  # Move 'HP Model Number' to the front\n",
    "        df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4bf43-fc09-48f1-a2c8-ca457af5eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_reviews = pd.DataFrame()\n",
    "for sku in sku_list:\n",
    "    print('Get review', sku)\n",
    "    data = hp_review(sku)\n",
    "    # print(data)\n",
    "    print('Review count',len(data))\n",
    "    if data is not None:\n",
    "        hp_reviews = pd.concat([hp_reviews,data],axis = 0)\n",
    "        print('All Review count',len(hp_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4240ac1-8892-4dad-bfd7-991b4a44e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_reviews['Retailer']= \"HP US\"\n",
    "hp_reviews['Review date'] = pd.to_datetime(hp_reviews['Review date'])\n",
    "hp_reviews['Review date'] = hp_reviews['Review date'].dt.tz_localize(None)\n",
    "\n",
    "new_column_order = ['Retailer', 'HP Model Number', 'Model', 'Review date', 'Review title', 'Review rating', 'Review content', \n",
    "                   'Syndicated source']\n",
    "\n",
    "hp_reviews = hp_reviews[new_column_order]\n",
    "\n",
    "from datetime import datetime, date  \n",
    "date = datetime.today().strftime('%Y%m%d')  \n",
    "hp_reviews.to_excel(f'scraped_reviews/HP_reviews_{date}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943b610-9379-444b-a725-dc9ddfb0ea81",
   "metadata": {},
   "source": [
    "## Combine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4944b73-c70b-4db6-bd7c-425d240e70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([amazon_review, bestbuy_review, staple, walmart, hp_reviews], ignore_index=True, sort=False)\n",
    "combined_df\n",
    "\n",
    "combined_df.to_excel(f'scraped_reviews/all_reviews_{date}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
